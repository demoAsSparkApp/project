{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a27c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Real-Time Streaming + Delta Lake in PySpark (IMDB Dataset)\n",
       "\n",
       "This notebook demonstrates:\n",
       "- Simulating streaming ingestion with IMDB rating data\n",
       "- Writing to **Delta Lake**\n",
       "- Using **Delta features** like time travel and vacuum\n",
       "- Exploring **streaming performance optimization**\n",
       "- Using **Spark Structured Streaming APIs**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üìò SECTION 1: Introduction\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(\"\"\"\n",
    "# Real-Time Streaming + Delta Lake in PySpark (IMDB Dataset)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Simulating streaming ingestion with IMDB rating data\n",
    "- Writing to **Delta Lake**\n",
    "- Using **Delta features** like time travel and vacuum\n",
    "- Exploring **streaming performance optimization**\n",
    "- Using **Spark Structured Streaming APIs**\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7df7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 02:37:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from src.config.spark_config import get_spark_session\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Start Spark session with proper configs\n",
    "spark = get_spark_session(app_name=\"PySpark_Optimization_Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c48d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_path = os.path.join(PROJECT_ROOT, \"data\", \"parquet_data\", \"name_basics.parquet\")\n",
    "ratings_path = os.path.join(PROJECT_ROOT, \"data\", \"parquet_data\", \"title_ratings.parquet\")\n",
    "\n",
    "df_name = spark.read.parquet(name_path)\n",
    "df_ratings = spark.read.parquet(ratings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59956526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------+---------+-----------------+-------------------+------------------+----------------+----------------+----------------+----------------+---------------+---------------+---------------+-------------------+\n",
      "|nconst   |primaryName    |birthYear|deathYear|primaryProfession|secondaryProfession|tertiaryProfession|knownForTitles_1|knownForTitles_2|knownForTitles_3|knownForTitles_4|full_name      |name_lower     |name_length    |ingested_at        |\n",
      "+---------+---------------+---------+---------+-----------------+-------------------+------------------+----------------+----------------+----------------+----------------+---------------+---------------+---------------+-------------------+\n",
      "|nm0000001|Fred Astaire   |1899     |1987     |actor            |miscellaneous      |producer          |tt0072308       |tt0050419       |tt0027125       |tt0031983       |Fred Astaire   |fred astaire   |Fred Astaire   |2025-06-20 02:07:51|\n",
      "|nm0000002|Lauren Bacall  |1924     |2014     |actress          |soundtrack         |archive_footage   |tt0037382       |tt0075213       |tt0117057       |tt0038355       |Lauren Bacall  |lauren bacall  |Lauren Bacall  |2025-06-20 02:07:51|\n",
      "|nm0000003|Brigitte Bardot|1934     |\\N       |actress          |music_department   |producer          |tt0057345       |tt0049189       |tt0056404       |tt0054452       |Brigitte Bardot|brigitte bardot|Brigitte Bardot|2025-06-20 02:07:51|\n",
      "|nm0000004|John Belushi   |1949     |1982     |actor            |writer             |music_department  |tt0072562       |tt0077975       |tt0080455       |tt0078723       |John Belushi   |john belushi   |John Belushi   |2025-06-20 02:07:51|\n",
      "|nm0000005|Ingmar Bergman |1918     |2007     |writer           |director           |actor             |tt0050986       |tt0069467       |tt0050976       |tt0083922       |Ingmar Bergman |ingmar bergman |Ingmar Bergman |2025-06-20 02:07:51|\n",
      "+---------+---------------+---------+---------+-----------------+-------------------+------------------+----------------+----------------+----------------+----------------+---------------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_name.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727b41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|     primaryName|birthYear|\n",
      "+----------------+---------+\n",
      "|         Gong Li|     1965|\n",
      "|       Brad Pitt|     1963|\n",
      "|Gillian Anderson|     1968|\n",
      "| Pamela Anderson|     1967|\n",
      "|Jennifer Aniston|     1969|\n",
      "+----------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Replace '\\N' with None (if not already handled at read time)\n",
    "df_name = df_name.withColumn(\n",
    "    \"birthYear\",\n",
    "    when(col(\"birthYear\") == \"\\\\N\", None).otherwise(col(\"birthYear\"))\n",
    ")\n",
    "\n",
    "# Cast birthYear to integer\n",
    "df_name = df_name.withColumn(\"birthYear\", col(\"birthYear\").cast(\"int\"))\n",
    "\n",
    "# Filter as required\n",
    "df_filtered = df_name.filter((col(\"birthYear\") >= 1960) & (col(\"primaryProfession\").isNotNull()))\n",
    "df_filtered.select(\"primaryName\", \"birthYear\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8795a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- Project (8)\n",
      "   +- BroadcastHashJoin Inner BuildRight (7)\n",
      "      :- Project (3)\n",
      "      :  +- Filter (2)\n",
      "      :     +- Scan parquet  (1)\n",
      "      +- BroadcastExchange (6)\n",
      "         +- Filter (5)\n",
      "            +- Scan parquet  (4)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [primaryName#19, birthYear#20, primaryProfession#22, knownForTitles_1#25]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/aryan/Desktop/project/data/parquet_data/name_basics.parquet]\n",
      "PushedFilters: [IsNotNull(primaryProfession), IsNotNull(knownForTitles_1)]\n",
      "ReadSchema: struct<primaryName:string,birthYear:string,primaryProfession:string,knownForTitles_1:string>\n",
      "\n",
      "(2) Filter\n",
      "Input [4]: [primaryName#19, birthYear#20, primaryProfession#22, knownForTitles_1#25]\n",
      "Condition : ((CASE WHEN (birthYear#20 = \\N) THEN false ELSE (cast(birthYear#20 as int) >= 1960) END AND isnotnull(primaryProfession#22)) AND isnotnull(knownForTitles_1#25))\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [primaryName#19, knownForTitles_1#25]\n",
      "Input [4]: [primaryName#19, birthYear#20, primaryProfession#22, knownForTitles_1#25]\n",
      "\n",
      "(4) Scan parquet \n",
      "Output [2]: [tconst#33, averageRating#34]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/aryan/Desktop/project/data/parquet_data/title_ratings.parquet]\n",
      "PushedFilters: [IsNotNull(tconst)]\n",
      "ReadSchema: struct<tconst:string,averageRating:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [tconst#33, averageRating#34]\n",
      "Condition : isnotnull(tconst#33)\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [2]: [tconst#33, averageRating#34]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=82]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [1]: [knownForTitles_1#25]\n",
      "Right keys [1]: [tconst#33]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [primaryName#19, averageRating#34]\n",
      "Input [4]: [primaryName#19, knownForTitles_1#25, tconst#33, averageRating#34]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [primaryName#19, averageRating#34]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPLAIN physical/optimized plans\n",
    "df_join = df_filtered.join(df_ratings, df_filtered.knownForTitles_1 == df_ratings.tconst)\n",
    "df_join.select(\"primaryName\", \"averageRating\").explain(mode=\"formatted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fb9867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_repart = df_join.repartition(8)\n",
    "# df_coalesce = df_join.coalesce(2)\n",
    "\n",
    "# print(\"üîÅ Repartition Partitions:\", df_repart.rdd.getNumPartitions())\n",
    "# print(\"üß© Coalesce Partitions:\", df_coalesce.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83cb081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cached = df_join.cache()\n",
    "# df_cached.count()  # triggers caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c66b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- primaryName: string (nullable = true)\n",
      " |-- birthYear: integer (nullable = true)\n",
      " |-- deathYear: string (nullable = true)\n",
      " |-- primaryProfession: string (nullable = true)\n",
      " |-- secondaryProfession: string (nullable = true)\n",
      " |-- tertiaryProfession: string (nullable = true)\n",
      " |-- knownForTitles_1: string (nullable = true)\n",
      " |-- knownForTitles_2: string (nullable = true)\n",
      " |-- knownForTitles_3: string (nullable = true)\n",
      " |-- knownForTitles_4: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- name_lower: string (nullable = true)\n",
      " |-- name_length: string (nullable = true)\n",
      " |-- ingested_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_name.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2de12304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+--------+\n",
      "|   tconst|  primaryName|averageRating|numVotes|\n",
      "+---------+-------------+-------------+--------+\n",
      "|tt0000001|   Carmencita|          5.7|    2163|\n",
      "|tt0000002|         NULL|          5.5|     296|\n",
      "|tt0000003|√âmile Reynaud|          6.5|    2217|\n",
      "|tt0000004|         NULL|          5.3|     189|\n",
      "|tt0000005|         NULL|          6.2|    2955|\n",
      "+---------+-------------+-------------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Optional: Filter nulls from join key\n",
    "df_name_filtered = df_name.filter(df_name.knownForTitles_1.isNotNull())\n",
    "\n",
    "# Broadcast the smaller dataframe (df_name)\n",
    "df_broadcast = df_ratings.join(\n",
    "    broadcast(df_name_filtered.select(\"primaryName\", \"knownForTitles_1\")),\n",
    "    df_ratings.tconst == df_name_filtered.knownForTitles_1,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_broadcast.select(\"tconst\", \"primaryName\", \"averageRating\", \"numVotes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e92323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
